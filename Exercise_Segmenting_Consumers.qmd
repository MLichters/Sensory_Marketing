---
title-block-banner: true
title: "Sensory Marketing & Product Innovation"
subtitle: "Exercise - Segmenting consumers based on sensory acceptance tests in sensory labs, immersive environments, and natural consumption settings"
author: 
  - name: 
      given: Marcel
      family: Lichters
      degrees: Prof. Dr.
      id: ML
      attributes:
      corresponding: true
    email: marcel.lichters@ovgu.de
    url: https://www.marketing.ovgu.de/
    orcid: 0000-0002-3710-2292
    corresponding: true
    affiliation: 
    - name: Otto von Guericke University Magdeburg
      city: Magdeburg
      state: Saxony-Anhalt
      country: Germany
      url: https://www.ovgu.de/en/
date: last-modified
date-format: full
csl: apa-old-doi-prefix.csl
citation:
  language: "en"
  type: post-weblog
  url: https://pub.ww.ovgu.de/lichters/smpi/exercise/Exercise_Segmenting_Consumers.html
  title: "Sensory Marketing & Product Innovation: Exercise - Segmenting consumers based on sensory acceptance tests in sensory labs, immersive environments, and natural consumption settings"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    code-fold: false
    code-link: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-tools: false
    code-copy: hover
    number-sections: true
    number-depth: 3
    citations-hover: true
    footnotes-hover: true
    embed-resources: true
    anchor-sections: true
    email-obfuscation: javascript
editor: source
bibliography: references.bib
editor_options: 
  chunk_output_type: console
google-scholar: true
copyright: 
  holder: Prof. Dr. Marcel Lichters
  year: 2025
  statement: Please cite the author Marcel Lichters!
license: 
  text: >
    CC BY-NC-ND 4.0, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 
  type: open-access
  url: https://creativecommons.org/licenses/by-nc-nd/4.0/
---

------------------------------------------------------------------------

<a title="Follow me on ResearchGate" href="https://www.researchgate.net/profile/Marcel_Lichters?cp=shp"><img src="https://www.researchgate.net/images/public/profile_share_badge.png" alt="Follow me on ResearchGate"/></a>

[Connect with me on Open Science Framework](https://osf.io/u7hyz/) \| [Contact me via LinkedIn](https://www.linkedin.com/in/lichters/)

It might be necessary to use right-click -\> open in a new browser window depending on your machine*.*

------------------------------------------------------------------------

R analysis script presenting the solutions for the exercise in Sensory Marketing regarding Lichters et al. [-@lichters2021].

The purpose of this script is not solely to answer the exercise question. Moreover, studying these scripts should help students become familiar with some aspects of working in *R*.

::: callout-tip
## If this grabs your attention

If this exercise captures your interest, please take a look at our master study programs at Otto von Guericke University in Magdeburg (Germany), by clicking on the logo:
:::

<a title="Faculty of Economics and Management at Otto-von-Guericke-University Magdeburg" href="https://www.ovgu.de/unimagdeburg/en/Master-p-48820.html"><img src="Signet_WW_3.jpg" alt="Faculty of Economics and Management at Otto-von-Guericke-University Magdeburg" width="600"/></a>

------------------------------------------------------------------------

# Learning objectives for the students

This exercise is designed to help students learn how to:

-   Work with data stored within the [Open Science Framework](https://osf.io/)

-   Analyze sensory product acceptance tests in R

-   Apply repeated measures analysis of variance (rANOVA) 

-   Apply post-hoc procedures



```{r}
#| include: false


#clear workspace
rm(list = ls())


#load packages
if (!"pacman" %in% rownames(installed.packages())) install.packages("pacman")

library(pacman)

pacman::p_load(haven, kableExtra, knitr, labelled, tidyverse, ggpubr, quarto, htmltools, pwr, compute.es, rstatix, osfr)


```

------------------------------------------------------------------------

# Loading packages {#sec-loading-packages}

::: callout-important
## Beware!

*R* is a context-sensitive language. Thus, 'data' will not be interpreted the same way as 'Data'.
:::

In *R* most functionality is provided by additional packages.\
Most of the packages are well-documented; see: <https://cran.r-project.org/>

1.  The code chunk below first evaluates if the package [pacman](https://cran.r-project.org/web/packages/pacman/index.html) [@pacman] has already been installed on your machine. If yes, the corresponding package will be loaded. If not, *R* will install the package.

2.  Alternatively, you can do this manually first by executing *install.packages("pacman")* and then *library(pacman)*.

3.  The second line then loads the package [pacman](https://cran.r-project.org/web/packages/pacman/index.html).

4.  The third line uses the function *p_load()* from the pacman package to install (if necessary) and loads all packages that we provide as arguments.

```{r}


    if (!"pacman" %in% rownames(installed.packages())) install.packages("pacman")

    library(pacman)

    pacman::p_load(tidyverse, osfr, rstatix)


```

::: {.callout-caution collapse="true"}
## Expand to learn more about calling functions

In all code chunks throughout this script, you can receive additional help on each used function by clicking on its name (or via right-click and then opening in a new browser tab). Alternatively, when coding, we can see which arguments a function understands by pressing 'F1' while setting the cursor to the function's name.
:::

------------------------------------------------------------------------

Here is the *R* session info, which gives you information on my machine, all loaded packages, and their version:

```{r  }

sessionInfo()


```

------------------------------------------------------------------------

# Task 5.1.

> Loading data from the Open Science Framework (OSF)

The article provides the link to a corresponding OSF project: <https://doi.org/10.17605/OSF.IO/VKZ4Y>. We can find all data sets and R-scripts accompanying the Food Quality and Preference manuscript here. [OSF](https://osf.io/) is a free web application that allows researchers to collaboratively store, manage, and share their research materials (e.g., data, code, protocols). Most work on OSF is organized around projects, which include a cloud-based storage bucket where files can be stored and organized into directories. 

Please navigate to the corresponding project using your web browser. We can see that the project contains a data *component* named *Data Sets and Analysis Scripts*. If we click on the *component*, we are provided with its own *URL* identifier, which is <https://osf.io/kst7p>. Click on the *Files tab*. This data component houses the product acceptance ratings for the cappuccino category in the data set named **Working dataset Cappuccino.csv** within the directory path **Data and analysis scripts/Main studies**.

To load the data set, we use functions provided by the [*osfr*](https://cran.r-project.org/web/packages/osfr/index.html) package [@osfr].

We first assess the corresponding OSF data component from *R*. For this purpose, we apply the function *osf_retrieve_node()*, which expects us to hand in a *url* to an existing resource. We assign the results of the function to an object named *OSF_project_segmenting_consumers*.

```{r}

OSF_project_segmenting_consumers <- osf_retrieve_node("https://osf.io/kst7p/")

```

The code chunk above opens a, so to say, table of contents for the corresponding OSF data component. Let's list all the files uploaded to the path **Data and analysis scripts/Main studies**. This is done by the *osf_ls_files()* function:

```{r}

osf_ls_files(OSF_project_segmenting_consumers, path = "Data and analysis scripts/Main studies")

```

This returns an *osf_tbl* object, which is the *data.frame*-like class the *osfr* package uses to represent items retrieved from OSF. This one contains eight rows, one for each file stored on the OSF data component. To download a local copy of the cappuccino data set (row 3, by the way), we use the *osf_download()* function. More specifically, we build a pipeline of commands: First, we hand over our list of files stored in the OSF file path to a function *filter()*. This function helps us select only the entry with the file named *Working dataset Cappuccino.csv*. We better use the notation **dplyr::filter()** to ensure that the *filter()* function from the [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html)-package [@dplyr] is used since multiple other packages provide functions with the same name. The result of this step then goes as an argument to the last command in the pipe, which is *osf_download()*.

```{r}

osf_ls_files(OSF_project_segmenting_consumers, path = "Data and analysis scripts/Main studies") %>%
  dplyr::filter(name == "Working dataset Cappuccino.csv") %>%
  osf_download(conflicts = "overwrite")

```

As a result, the data set *Working dataset Cappuccino.csv* is stored in the current working directory of your *R*-project. All we need to do is to import it to our current *R* analysis session. For this purpose, we use *read.csv()* function. We assign the data set to a new object, **d**. In the last step, we convert the object to the [tibble] (https://tibble.tidyverse.org/) format, which is prominent among marketing researchers.

```{r}

d <- read.csv("Working dataset Cappuccino.csv")

d <- as_tibble(d)

```

Finally, we can give a brief overview of the imported data:

```{r}

d

```

::: callout-tip
## Interpretation

The data set contains one row for each of the n=104 interviewed consumers. Furthermore, there are 13 variables as columns. The first variable houses the consumer ID, whereas the remaining columns comprise the consumers' overall liking judgments ("OL") about the different product samples (scaled from 1 to 9) across the varying testing environments. 
The abbreviations "Bar", "Lab", and "VR" thereby stand for the environments **Real Coffeehouse**,**Sensory Lab**, and **Immersive Coffeehouse**. The abbreviations "Bar.1shot", "Bar.2shot", "Siz.1shot", and "Siz.2shot" mark the different product samples, namely cappuccinos made of varying types of coffee beans ("Barista" and "Sizilianer Art", according to the authors), and prepared with one or two shots of that coffee.
:::


------------------------------------------------------------------------

# Task 5.2.

> Apply a repeated measures ANOVA to the overall liking scores to evaluate if you can also find a significant difference in mean product acceptance across test environments (a so-called **level effect**).


Here, we are asked to replicate the original analysis proposed by the authors. Thus, the easiest way to solve the task is to look at the analysis scripts provided on OSF. In the chunk below, however, we apply our own code.

We first tell *R* to handle the first column, *"ID"* as a categorical factor variable instead of a numerical one.

Next, we use the function [*pivot_longer*](https://tidyr.tidyverse.org/reference/pivot_longer.html) to restructure our data set from the **wide format** (each consumer is one row) to the **long format** (each product judgment is one row). Thus, we are *stacking* the data. We are doing so because the subsequent repeated measures ANOVA (rANOVA) expects the long instead of the wide format. Within the function call, we hand in *d* for the data argument. Further, we tell the function to use the columns named "OL.Bar.1shot...Lab" to "OL.Siz.2shot...Fil" for creating the longer format. The *names_to* argument tells the function which name to use for the new stacked variable collecting all product variant names. Finally, the *values_to* argument sets the name for the stacked product evaluations. The results are stored in a new object called *d_stacked*.

```{r}

d$ID <- as_factor(d$ID)


d_stacked <- pivot_longer(data = d, 
                          cols = OL.Bar.1shot...Lab:OL.Siz.2shot...Fil, 
                          names_to = "product_sample", 
                          values_to = "liking")



print(d_stacked, n = 14)


```



::: callout-tip
## Interpretation

In the resultant long-format dataset, each consumer now has 12 rows since each consumer has tasted four different product formulations within three different consumption environments.
:::


Bis hier gekommen


For the subsequent rANOVA, we need to create additional columns indicating the different levels of the experimental factors of the study. This is done by the code chunk below. Here, we use a combination of the functions *mutate()*, *case_when()*, and *str_detect()* to create two new variables in d_stacked. For example, in the first code block, we feed in d_stacked to [*mutate*](https://dplyr.tidyverse.org/reference/mutate.html "See how mutate() is function")*().* This function is a generic one, which can add new variables and preserve existing ones. Next, we must tell *mutate()* a certain build logic for the new variable *sample*. Here, we use [*case_when()*](https://dplyr.tidyverse.org/reference/case_when.html). Within *case_when()*, we can apply additional logic. The function then applies this logic to each row in d_stacked. Let's focus on one example **str_detect(product_sample, "Bar.1shot") \~ 'Bar_1shot'**. We use [*str_detect()*](https://cran.r-project.org/web/packages/stringr/index.html) from the *stringr* package [@stringr] to evaluate for each line of the variable *product_sample* whether it contains the string "Bar.1shot". If so, the function sets the value 'Bar_1shot' to the newly created variable *sample*. If no match is detected, no value will be set. In the end, we create two new variables, *sample,* and *environment*, which indicate for each row in d_stacked, where a consumer has evaluated which cappuccino formula.

The last two lines tell *R* to handle these variables as categorical factors.

```{r}

d_stacked <-  d_stacked %>% mutate(
  sample = case_when(
  str_detect(product_sample, "Bar.1shot") ~ 'Bar_1shot',
  str_detect(product_sample, "Siz.1shot") ~ 'Siz_1shot',
  str_detect(product_sample, "Siz.2shot") ~ 'Siz_2shot',
  str_detect(product_sample, "Bar.2shot") ~ 'Bar_2shot'
  ))

d_stacked <-  d_stacked %>% mutate(
  environment = case_when(
  str_detect(product_sample, "Lab") ~ 'Sensory_Lab',
  str_detect(product_sample, "VR") ~ 'Immersive_Coffeehouse',
  str_detect(product_sample, "Fil") ~ 'Real_Coffeehouse'
  ))

d_stacked$sample <- as_factor(d_stacked$sample)
d_stacked$environment <- as_factor(d_stacked$environment)

print(d_stacked)

```

Now, we are ready to conduct the rANOVA. For this purpose, we take advantage of the *anova_test()* function, which the [*rstatix*](https://cran.r-project.org/web/packages/rstatix/index.html) package provides. We need to provide four arguments in our case:

-   data = the data set to use
-   dv = the dependent variable in the rANOVA (the variable *liking* in the present case)
-   wid = the variable indicating the case IDs of consumers (*ID* in our case)
-   within = a vector of variable names, for which the analysis assumes within-subjects observations

We assign the results of our rANOVA to a new object named *cappuccino_rANOVA*.

Lastly, we print the results while limiting the display to 3 digits.

```{r}

cappuccino_rANOVA <- anova_test(data = d_stacked,
                                dv = liking,
                                wid = ID,
                                within = c(sample, environment))
                                

print(cappuccino_rANOVA, digits = 3)


```

In essence, the results are presented in 3 tables.

The first one contains the main results of the rANOVA. We can see that three effects were tested: (1) The main effect of the *sample* (i.e., the different cappuccino recipes), (2) the main effect of the *environment* (i.e., the difference in mean acceptability scores across consumption environments), and (3) the interaction of sample and environment (i.e., are different cappuccino recipes evaluated differently across environments). For example, one sample might be evaluated best in the sensory lab but worst in the real coffeehouse).

::: callout-tip
## Main results

We see that the main effects of *sample* and *environment* are both significant as the empirical p-values fall below 5%. Thus, we can replicate the authors' findings regarding the "level effect" across consumption environments.
:::

More precisely, the main effect of the *environment* $F_{2, 206}$ = `r round(cappuccino_rANOVA$ANOVA$F[2], digits = 2)`, p = `r round(cappuccino_rANOVA$ANOVA$p[2], digits = 3)` is significant with an effect size of generalized $\eta^{2}_{p}$ = `r round(cappuccino_rANOVA$ANOVA$ges[2], digits = 2)`, a small effect [@olejnik2003; @bakeman2005]. This perfectly mirrors the results reported in the Web Appendix, even though the Web Appendix obviously reported the wrong value for the generalized $\eta^{2}_{p}$ (0.10 instead of 0.01).

The two other tables reported above simply test one of the assumptions for rANOVA, namely whether the data exerts [*Sphericity*](https://statistics.laerd.com/statistical-guides/sphericity-statistical-guide.php) across measurement times [@Field.2012, p. 551]. To make it short, the second table shows that this assumption is significantly violated for the main effect of the *sample* but not for the other effects under consideration. If the assumption is violated, one must consider the third table. Here, we can find two proposed correction methods (Hyunh-Feldt \[HF\] and Greenhouse-Geisser \[GG\]), of which the latter is most commonly applied [@Field.2012, p. 551]. However, we see that both main effects' p-values also fall below 5% with the Greenhouse-Geisser correction (column *p\[GG\]*).

------------------------------------------------------------------------

# Task 5.3.

::: callout-note
## Your task

Use a different post-hoc test than the authors. Based on your chosen test, is the immersive environment still significantly different from the other two?
:::

On page 4 of the Web Appendix, the authors state: *"\[...\] running a series of repeated measures analysis of variance analyses (rANOVAs) combined with post hoc Bonferroni-corrected within-subjects t-tests".*

Thus, in the article, the authors applied the Bonferroni correction [@Bonferroni.1936] to account for multiple group comparisons. Researchers in statistics have proposed countless [alternatives for post-hoc tests](https://www.statology.org/anova-post-hoc-tests/) to account for family-wise Alpha error when comparing multiple groups (in our case, the three environments). For illustration, we choose Holm's method [@Holm.1979] for the code chunk below, which is less conservative (i.e., differences get significant faster) than Bonferroni.

We combine the two functions *with()* and *pairwise.t.test()* in the code below and assign the results of our post-hoc procedure to a new object named *post_hoc*. We first submit d_stacked to the *with()* function, which simply tells *R* to conduct several analysis steps with it. The specific analysis is specified next. Here, we call for a t-test within the function *pairwise.t.test()*. This function expects us to provide four arguments:

-   x = the dependent variable in the test (the product liking in our case)
-   g = the variable forming the groups in the pair-wise comparisons (environment in the present task)
-   p.adjust.method = the post-hoc procedure to apply to control for multiple comparisons (try *?p.adjust()* to see which options are available)
-   paired = a logical indicator flagged as TRUE to highlight that the observations are not independent (the same consumers evaluated the same products within different environments. Thus, a paired t-test is applied)

```{r}



post_hoc <- d_stacked %>% 
  with(., pairwise.t.test(x=liking, 
                          g=environment, 
                          p.adjust.method="holm", 
                          paired=TRUE))

print(post_hoc, digits = 3)



```

The resultant matrix presents the p-values of the 3 three pair-wise comparisons. We see that at Alpha=5%, the mean product likings of the immersive coffeehouse condition significantly differ from the sensory lab and the real coffeehouse. Therefore, the reported results of the article are robust, even when using Holm's correction instead of Bonferroni.

------------------------------------------------------------------------

# Task 5.4.

::: callout-note
## Your task

Only focus on the real coffeehouse environment! If you choose a different post-hoc test as compared to the authors, do you find the same significant differences between products?
:::

This task is very similar to the previous one. The difference is that we have to focus only at the real coffeehouse environment. Furthermore, we are investigating post-hoc tests for the *sample* factor instead of the environment factor. As an alternative to the authors' Bonferroni procedure, we, again, draw on Holm's post-hoc test.

In the code chunk below, we, again, use dplyr's *filter()* function to select only the rows in *d_stacked* that correspond to observations made in the real coffeehouse environment. These cases then move on to the same code we have applied above. This time, however, the argument g is formed by the sample variable, which contains information on the different cappuccino products.

```{r}


post_hoc_real_coffeehouse <- d_stacked %>% 
  dplyr::filter(environment == "Real_Coffeehouse") %>%
  with(., pairwise.t.test(
    x=liking, 
    g=sample, 
    p.adjust.method="holm", 
    paired=TRUE))


print(post_hoc_real_coffeehouse, digits = 3)

```

The resultant matrix presents the p-values of the four pair-wise comparisons. We see that at Alpha=5%, the acceptability scores of all product variants are significantly different from each other, except for the tests between *Bar_1shot and Siz_1shot*, as well as between *Bar_2shot and Siz_2shot*. In conclusion, applying Holm's instead of Bonferroni's post-hoc correction does not change the interpretation of the data within the real coffeehouse condition.

------------------------------------------------------------------------
